package com.linkedin.databus2.producers.db;
/*
 * Copyright 2013 LinkedIn Corp. All rights reserved
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */


import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.util.List;
import java.util.regex.Pattern;

import org.apache.avro.Schema;
import org.apache.avro.Schema.Field;
import org.apache.avro.Schema.Type;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericDatumWriter;
import org.apache.avro.generic.GenericRecord;
import org.apache.avro.io.BinaryEncoder;
import org.apache.avro.io.Encoder;
import org.apache.hadoop.hbase.KeyValue;
import org.apache.log4j.Logger;

import com.linkedin.databus.core.DbusEventBufferAppendable;
import com.linkedin.databus.core.DbusEventKey;
import com.linkedin.databus.core.UnsupportedKeyException;
import com.linkedin.databus.core.monitoring.mbean.DbusEventsStatisticsCollector;
import com.linkedin.databus2.producers.EventCreationException;
import com.linkedin.databus2.producers.PartitionFunction;
import com.linkedin.databus2.relay.config.ReplicationBitSetterStaticConfig;
import com.linkedin.databus2.relay.config.ReplicationBitSetterStaticConfig.SourceType;
import com.linkedin.databus2.schemas.utils.SchemaHelper;

/**
 * Reusable EventFactory which can generate Avro serialized records based on an Avro schema file
 * and using Avro's GenericData facilities.
 *
 * This EventFactory requires specific metadata be included in the Avro schema file, including
 * database field names and column indexes. Ideally the schema is generated by the
 * Databus2 SchemaGenerator command-line tool, which generates this metadata automatically.
 */
public class HbaseEventFactory
implements EventFactory
{
  /** Avro schema for the generated events. */
  protected final Schema _eventSchema;

  /** Unique schema ID for the _eventSchema. */
  protected final byte[] _schemaId;

  /** Source ID for this event source. */
  protected final short _sourceId;

  /** Physical source id */
  protected final short _pSourceId;

  /** PartitionFunction used to generate the event partition based on event key. */
  protected final PartitionFunction _partitionFunction;

  /** Logger for error and debug messages. */
  private final Logger _log = Logger.getLogger(getClass());

  /** key column name. */
  protected String keyColumnName = "key";

  /** Replication BitSetter StaticConfig **/
  private final ReplicationBitSetterStaticConfig _replSetterConfig;

  private final Pattern _replBitSetterPattern;

  public static final String MODULE = HbaseEventFactory.class.getName();
  public static final Logger LOG = Logger.getLogger(MODULE);

  public HbaseEventFactory(short sourceId, short pSourceId,
                                       String eventSchema, PartitionFunction partitionFunction,
                                       ReplicationBitSetterStaticConfig replSetterConfig)
  throws EventCreationException, UnsupportedKeyException
  {
    // TODO: Constructor should validate that eventSchema has all required metadata
    _sourceId = sourceId;
    _pSourceId = pSourceId;
    _eventSchema = Schema.parse(eventSchema);
    _schemaId = SchemaHelper.getSchemaId(eventSchema);
    _partitionFunction = partitionFunction;

    _replSetterConfig = replSetterConfig;
    //something about to identify a record is a replicated..
    if ((null != _replSetterConfig) && (SourceType.COLUMN.equals(_replSetterConfig.getSourceType())))
      _replBitSetterPattern = Pattern.compile(replSetterConfig.getRemoteUpdateValueRegex());
    else
      _replBitSetterPattern = null;

    String keyNameOverride = SchemaHelper.getMetaField(_eventSchema, "key");
    if (null != keyNameOverride)
    {
      keyColumnName = keyNameOverride;
      _log.info(_eventSchema.getFullName() + ": using primary key override:" + keyColumnName);
    }

    // Examine the event schema to determine the proper type of "key". The key must be a
    // String, int, or long data type.
    Field keyField = _eventSchema.getField(keyColumnName);

    if(keyField == null)
    {
      throw new EventCreationException("The event schema is missing the required field \"key\".");
      //schema need a key..
    }
  }

  protected byte[] serializeEvent(GenericRecord record,
                                  long scn,
                                  long timestamp,
                                  KeyValue row,
                                  DbusEventBufferAppendable eventBuffer,
                                  boolean enableTracing,
                                  DbusEventsStatisticsCollector dbusEventsStatisticsCollector)
  throws EventCreationException, UnsupportedKeyException
  {
    // Serialize the row
    byte[] serializedValue;
    try
    {
      ByteArrayOutputStream bos = new ByteArrayOutputStream();
      Encoder encoder = new BinaryEncoder(bos);
      GenericDatumWriter<GenericRecord> writer = new GenericDatumWriter<GenericRecord>(record.getSchema());
      writer.write(record, encoder);
      serializedValue = bos.toByteArray();
    }
    catch(IOException ex)
    {
      throw new EventCreationException("Failed to serialize the Avro GenericRecord. ResultSet was: (" + row + ")", ex);
    }
    catch(RuntimeException ex)
    {
      // Avro likes to throw RuntimeExceptions instead of checked exceptions when serialization fails.
      throw new EventCreationException("Failed to serialize the Avro GenericRecord. ResultSet was: (" + row + ")", ex);
    }
    return serializedValue;
  }
  /*
   * @see com.linkedin.databus2.monitors.db.EventFactory#createEvent(long, long, java.sql.ResultSet)
   */
  @Override
  public long createAndAppendEvent(long scn,
                                   long timestamp,
                                   KeyValue row,
                                   DbusEventBufferAppendable eventBuffer,
                                   boolean enableTracing,
                                   DbusEventsStatisticsCollector dbusEventsStatisticsCollector)
  throws EventCreationException, UnsupportedKeyException
  {
    // Serialize the row into an Avro GenericRecord
    GenericRecord record = buildGenericRecord(row);
    boolean isReplicated = isReplicatedEvent(row);
    return createAndAppendEvent(scn, timestamp, record, row, eventBuffer, enableTracing,
                                isReplicated, dbusEventsStatisticsCollector);
  }
  private boolean isReplicatedEvent(KeyValue row)
  {
	  return false;
//		TODO i don't know what to do here,so always false now.
//	    boolean replicated = false;
//	    try
//	    {
//	      if ((null != row) &&
//	          (_replSetterConfig != null) &&
//	          (SourceType.COLUMN.equals(_replSetterConfig.getSourceType())))
//	      {
//	        String value = row.getString(_replSetterConfig.getFieldName());
//	
//	        if ((null != value) && (null != _replBitSetterPattern))
//	        {
//	          replicated =_replBitSetterPattern.matcher(value).matches();
//	        }
//	      }
//	    }
//	    catch (SQLException sqlEx)
//	    {
//	      LOG.error("Unable to identify if this row was externally replicated. Config Used :" + _replSetterConfig,sqlEx);
//	      throw sqlEx;
//	    }
//	    return replicated;
  }

  @Override
  public long createAndAppendEvent(long scn, long timestamp,
                                   GenericRecord record, DbusEventBufferAppendable eventBuffer,
                                   boolean enableTracing,
                                   DbusEventsStatisticsCollector dbusEventsStatisticsCollector)
  throws EventCreationException, UnsupportedKeyException
  {
    return createAndAppendEvent(scn, timestamp, record, null, eventBuffer, enableTracing,
                                false, dbusEventsStatisticsCollector);
  }

  public long createAndAppendEvent(long scn,
                                   long timestamp,
                                   GenericRecord record,
                                   KeyValue row,
                                   DbusEventBufferAppendable eventBuffer,
                                   boolean enableTracing,
                                   boolean isReplicated,
                                   DbusEventsStatisticsCollector dbusEventsStatisticsCollector)
  throws EventCreationException, UnsupportedKeyException
  {
    byte[] serializedValue = serializeEvent(record, scn, timestamp, row, eventBuffer, enableTracing,
                                            dbusEventsStatisticsCollector);
    // Append the event to the databus event buffer
    DbusEventKey eventKey = new DbusEventKey(record.get(keyColumnName));
    short lPartitionId = _partitionFunction.getPartition(eventKey);
    eventBuffer.appendEvent(eventKey, _pSourceId, lPartitionId, timestamp * 1000000, _sourceId,
                            _schemaId, serializedValue, enableTracing, isReplicated, dbusEventsStatisticsCollector);
    return serializedValue.length;
  }
  protected GenericRecord buildGenericRecord(KeyValue kv) //exchange one KeyValue into a GenericRecord
  throws EventCreationException
  {
    boolean traceEnabled = _log.isTraceEnabled();
    if (traceEnabled)
    {
      _log.trace("--- New Record For Hbase KeyValue ---");
    }
    GenericRecord record = new GenericData.Record(_eventSchema);
    List<Field> fields = _eventSchema.getFields();
    //TODO one record only has two fields called key and value...
    for (Field field : fields)
    {
      String databaseFieldName = SchemaHelper.getMetaField(field, "dbFieldName");
      byte[] databaseFieldValue;
      if (databaseFieldName.equals("key")) {
    	  databaseFieldValue = kv.getKey();
      } else {
    	  databaseFieldValue = kv.getValue();
      }
      put(record, field, databaseFieldValue);
    }
    return record;
  }
  private void putSimpleValue(GenericRecord record,
                              String schemaFieldName,
                              Type avroFieldType,
                              Object databaseFieldValue)
  throws EventCreationException
  {
    assert null != databaseFieldValue;

    switch(avroFieldType)
    {
      case BYTES:
        if (databaseFieldValue instanceof byte[])
        {
          record.put(schemaFieldName, ByteBuffer.wrap((byte[]) databaseFieldValue));
        }
        break;
      case NULL:
        record.put(schemaFieldName, null);
        break;
      default:
        throw new EventCreationException("unknown simple type " + avroFieldType.toString() +
                                         " for field " + schemaFieldName);
    }
  }

  private void put(GenericRecord record, Field field, byte[] databaseFieldValue)
  throws EventCreationException
  {
    // Get the field name and type from the event schema
    String schemaFieldName = field.name();
    Schema fieldSchema = SchemaHelper.unwindUnionSchema(field);  // == field.schema() if not a union
    Type avroFieldType = fieldSchema.getType();
    if (databaseFieldValue != null)
    {
      if (_log.isTraceEnabled())
      {
        _log.trace("record.put(\"" + schemaFieldName + "\", (" + avroFieldType + ") \"" + databaseFieldValue + "\"");
      }

      try
      {
        switch (avroFieldType)
        {
          case BYTES:
          case NULL:
            putSimpleValue(record, schemaFieldName, avroFieldType, databaseFieldValue);
            break;
          default:
            throw new EventCreationException("Don't know how to populate this type of field: " + avroFieldType);
        }
      }
      catch(ClassCastException ex)
      {
        throw new EventCreationException("Type conversion error for field name (" + field.name() +
                                         ") in source " + _sourceId + ". Value was: " + databaseFieldValue +
                                         " avro field was: " + avroFieldType, ex);
      }
    }
  }
  public long createAndAppendEvent(long scn,
          long timestamp,
          ResultSet row,
          DbusEventBufferAppendable eventBuffer,
          boolean enableTracing,
          DbusEventsStatisticsCollector dbusEventsStatisticsCollector)
	throws SQLException, EventCreationException, UnsupportedKeyException
	{
	  /*
	   * TODO throw an Unsupported Exception
	   */
	  return -1;
	}
}
